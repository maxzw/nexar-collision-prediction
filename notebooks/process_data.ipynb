{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1500, 4)\n",
      "Test shape: (1344, 1)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/raw/test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = YOLO(\"yolov8m-seg.pt\")\n",
    "model.to(device)\n",
    "vehicle_classes = {\"car\", \"truck\", \"bus\", \"motorbike\", \"bicycle\"}\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_mask(frame: np.ndarray, model: YOLO) -> np.ndarray | None:\n",
    "    \"\"\"Extracts vehicle masks from a given frame using the YOLO model.\n",
    "\n",
    "    If no vehicle masks are found, returns None.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    masks = results.masks\n",
    "    classes = results.boxes.cls\n",
    "\n",
    "    if masks is None or masks.data is None or len(masks.data) == 0:\n",
    "        return None\n",
    "\n",
    "    H, W = frame.shape[:2]\n",
    "    mask_out = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    # Convert once\n",
    "    masks_np = masks.data.cpu().numpy().astype(np.uint8)\n",
    "    classes_np = classes.cpu().numpy()\n",
    "\n",
    "    for seg, cls_id in zip(masks_np, classes_np):\n",
    "        class_name = model.model.names[int(cls_id)]\n",
    "        if class_name in vehicle_classes:\n",
    "            mask = cv2.resize(seg, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "            mask_out[mask > 0] = 255\n",
    "\n",
    "    return mask_out[None, ...]  # Shape: (1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flow_channels(frame1: np.ndarray, frame2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute optical flow channels between two frames.\n",
    "\n",
    "    Flow is returned as a 2-channel image with shape (2, H, W):\n",
    "    - Channel 0: Magnitude of flow\n",
    "    - Channel 1: Angle of flow\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    prev_gray = cv2.cvtColor(frame1, cv2.COLOR_RGB2GRAY)\n",
    "    next_gray = cv2.cvtColor(frame2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Compute Dense Optical Flow (Farneback)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Compute magnitude and direction\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # Normalize magnitude and angle\n",
    "    magnitude = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    angle = (angle * 180 / np.pi / 2).astype(np.uint8)\n",
    "\n",
    "    # Stack into a single 2-channel image\n",
    "    flow_channels = np.dstack((magnitude, angle))\n",
    "\n",
    "    # Reorder dimensions to (2, H, W)\n",
    "    return np.moveaxis(flow_channels, -1, 0)  # Shape: (2, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_image(image: np.ndarray, factor: int) -> np.ndarray:\n",
    "    \"\"\"Downsample imput image by a given factor.\n",
    "\n",
    "    Accepts images in both (C, H, W) and (H, W, C) formats, but returns in (C, H, W) format.\n",
    "    \"\"\"\n",
    "    if image.shape[0] <= 3:\n",
    "        # If the image is in (C, H, W) format, transpose to (H, W, C) for resizing\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    # Downsample the image\n",
    "    h, w = image.shape[:2]\n",
    "    downsampled_image = cv2.resize(image, (w // factor, h // factor), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # If downsampled image now has two dimensions, convert to 1 channels (H, W, 1)\n",
    "    if len(downsampled_image.shape) == 2:\n",
    "        downsampled_image = np.expand_dims(downsampled_image, axis=-1)\n",
    "\n",
    "    # Make sure to resize back to (C, H, W) format\n",
    "    return np.transpose(downsampled_image, (2, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process negative training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [2:05:10<00:00, 10.01s/it]  \n"
     ]
    }
   ],
   "source": [
    "SAVE_EVERY = 30  # Every 1 second @30 FPS\n",
    "\n",
    "dq = deque(maxlen=3)\n",
    "for row in tqdm(train_df[train_df[\"target\"] == 0].to_dict(orient=\"records\")):\n",
    "    # Load video\n",
    "    video_path = f\"../data/raw/train/{str(row['id']).zfill(5)}.mp4\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Iterate frames\n",
    "    frame_index = 0\n",
    "    save_index = 0\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        # Extract information if required\n",
    "        if frame_index % SAVE_EVERY == 0 and len(dq) == 3:\n",
    "            mask = extract_mask(frame, model)           # Shape: (1, H, W)\n",
    "            flow = compute_flow_channels(dq[0], frame)  # Shape: (2, H, W)\n",
    "\n",
    "            flow_downsampled = downsample_image(flow, 3)\n",
    "            frame_downsampled = downsample_image(frame, 3)\n",
    "\n",
    "            save_dir = f\"../data/processed/train/{str(row['id']).zfill(5)}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            flow_path = save_dir + f\"/flows/{str(save_index).zfill(2)}.pt\"\n",
    "            os.makedirs(os.path.dirname(flow_path), exist_ok=True)\n",
    "            torch.save(torch.tensor(flow_downsampled).to(dtype=torch.float32), flow_path)\n",
    "\n",
    "            frame_path = save_dir + f\"/frames/{str(save_index).zfill(2)}.pt\"\n",
    "            os.makedirs(os.path.dirname(frame_path), exist_ok=True)\n",
    "            torch.save(torch.tensor(frame_downsampled).to(dtype=torch.int16), frame_path)\n",
    "\n",
    "            # Save mask if it exists\n",
    "            if mask is not None:\n",
    "                mask_downsampled = downsample_image(mask, 3)\n",
    "                mask_path = save_dir + f\"/masks/{str(save_index).zfill(2)}.pt\"\n",
    "                os.makedirs(os.path.dirname(mask_path), exist_ok=True)\n",
    "                torch.save(torch.tensor(mask_downsampled).to(dtype=torch.int16), mask_path)\n",
    "\n",
    "            save_index += 1\n",
    "\n",
    "        frame_index += 1\n",
    "        dq.append(frame)\n",
    "\n",
    "    video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process positive training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [2:24:00<00:00, 11.52s/it]  \n"
     ]
    }
   ],
   "source": [
    "FPS = 30  # Frames per second\n",
    "\n",
    "dq = deque(maxlen=3)\n",
    "for row in tqdm(train_df[train_df[\"target\"] == 1].to_dict(orient=\"records\")):\n",
    "    # Load video\n",
    "    video_path = f\"../data/raw/train/{str(row['id']).zfill(5)}.mp4\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    time_alert = float(row[\"time_of_alert\"])\n",
    "    time_event = float(row[\"time_of_event\"])\n",
    "    start_frame = math.floor(time_alert * FPS)\n",
    "    end_frame = math.ceil(time_event * FPS)\n",
    "\n",
    "    # Seek to start frame\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    frame_index = start_frame\n",
    "    save_index = 0\n",
    "\n",
    "    while frame_index <= end_frame:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        dq.append(frame)\n",
    "\n",
    "        if len(dq) == 3:\n",
    "            mask = extract_mask(frame, model)           # Shape: (1, H, W)\n",
    "            flow = compute_flow_channels(dq[0], frame)  # Shape: (2, H, W)\n",
    "\n",
    "            flow_downsampled = downsample_image(flow, 3)\n",
    "            frame_downsampled = downsample_image(frame, 3)\n",
    "\n",
    "            save_dir = f\"../data/processed/train/{str(row['id']).zfill(5)}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            flow_path = save_dir + f\"/flows/{str(save_index).zfill(2)}.pt\"\n",
    "            os.makedirs(os.path.dirname(flow_path), exist_ok=True)\n",
    "            torch.save(torch.tensor(flow_downsampled).to(dtype=torch.float32), flow_path)\n",
    "\n",
    "            frame_path = save_dir + f\"/frames/{str(save_index).zfill(2)}.pt\"\n",
    "            os.makedirs(os.path.dirname(frame_path), exist_ok=True)\n",
    "            torch.save(torch.tensor(frame_downsampled).to(dtype=torch.int16), frame_path)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask_downsampled = downsample_image(mask, 3)\n",
    "                mask_path = save_dir + f\"/masks/{str(save_index).zfill(2)}.pt\"\n",
    "                os.makedirs(os.path.dirname(mask_path), exist_ok=True)\n",
    "                torch.save(torch.tensor(mask_downsampled).to(dtype=torch.int16), mask_path)\n",
    "\n",
    "            save_index += 1\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1344/1344 [16:33<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "FRAMES_NEEDED = 6  # 3 for optical flow context + 3 to process\n",
    "\n",
    "dq = deque(maxlen=3)\n",
    "for row in tqdm(test_df.to_dict(orient=\"records\")):\n",
    "    video_path = f\"../data/raw/test/{str(row['id']).zfill(5)}.mp4\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get total number of frames\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    start_frame = max(0, total_frames - FRAMES_NEEDED)\n",
    "\n",
    "    # Seek to frame N - 6\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    frame_index = start_frame\n",
    "    save_index = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        dq.append(frame)\n",
    "\n",
    "        # Only save the *last 3 frames* (from N-3 to N-1)\n",
    "        if frame_index >= total_frames - 3 and len(dq) == 3:\n",
    "            mask = extract_mask(frame, model)           # Shape: (1, H, W)\n",
    "            flow = compute_flow_channels(dq[0], frame)  # Shape: (2, H, W)\n",
    "\n",
    "            flow_downsampled = downsample_image(flow, 3)\n",
    "            frame_downsampled = downsample_image(frame, 3)\n",
    "\n",
    "            save_dir = f\"../data/processed/test/{str(row['id']).zfill(5)}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            flow_path = save_dir + f\"/flows/{str(save_index).zfill(2)}.pt\"\n",
    "            os.makedirs(os.path.dirname(flow_path), exist_ok=True)\n",
    "            torch.save(torch.tensor(flow_downsampled).to(dtype=torch.float32), flow_path)\n",
    "\n",
    "            frame_path = save_dir + f\"/frames/{str(save_index).zfill(2)}.pt\"\n",
    "            os.makedirs(os.path.dirname(frame_path), exist_ok=True)\n",
    "            torch.save(torch.tensor(frame_downsampled).to(dtype=torch.int16), frame_path)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask_downsampled = downsample_image(mask, 3)\n",
    "                mask_path = save_dir + f\"/masks/{str(save_index).zfill(2)}.pt\"\n",
    "                os.makedirs(os.path.dirname(mask_path), exist_ok=True)\n",
    "                torch.save(torch.tensor(mask_downsampled).to(dtype=torch.int16), mask_path)\n",
    "\n",
    "            save_index += 1\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to features\n",
    "train_df[\"features_path\"] = train_df[\"id\"].apply(lambda x: f\"../data/processed/train/{str(x).zfill(5)}\")\n",
    "test_df[\"features_path\"] = test_df[\"id\"].apply(lambda x: f\"../data/processed/test/{str(x).zfill(5)}\")\n",
    "\n",
    "# Add number of frames (for sampling)\n",
    "train_df[\"n_frames\"] = train_df[\"features_path\"].apply(lambda x: len(os.listdir(x + \"/frames\")))\n",
    "test_df[\"n_frames\"] = test_df[\"features_path\"].apply(lambda x: len(os.listdir(x + \"/frames\")))\n",
    "\n",
    "# Save to parquet\n",
    "train_df.to_parquet(\"../data/processed/train.parquet\", index=False)\n",
    "test_df.to_parquet(\"../data/processed/test.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
